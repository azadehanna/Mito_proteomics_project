{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cf0fd8-c566-4f4b-b00b-6c2fb4de57cc",
   "metadata": {},
   "source": [
    "# read file contains All proteins to start pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3752add1-8dda-4858-8207-7fb1ee9bf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All libraries needed for this step\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3c287-0dcf-40fa-8540-eb618050418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8b971-2a1b-45e0-b944-e3fda76083a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file path \n",
    "file_path = \"./total_protein_1.csv\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Print success message and show the first few rows\n",
    "print(data.head())\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "\n",
    "# total number of rows and columns\n",
    "rows, columns = data.shape\n",
    "\n",
    "print(f\"Rows: {rows}, Columns: {columns}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Select columns from column 7 to column 33 (group for samples)\n",
    "selected_columns = data.iloc[:, 6:33]  \n",
    "\n",
    "# Get the number of selected columns\n",
    "total_selected_columns = selected_columns.shape[1]\n",
    "\n",
    "print(f\"Total selected columns: {total_selected_columns}\")\n",
    "print(f\"Selected column names: {list(selected_columns.columns)}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Extract columns that represent samples using a regular expression\n",
    "sample_columns = [col for col in data.columns if re.match(r'^(WtSham|WtLPS|WtLPSTB|WtShamTB|BtgSham|BtgLPS|BKSham|BKLPS)_\\d+', col)]\n",
    "\n",
    "# Count total number of samples\n",
    "total_samples = len(sample_columns)\n",
    "\n",
    "# Count the number of repeats for each group\n",
    "group_counts = pd.Series([re.match(r'^[a-zA-Z]+', col).group(0) for col in sample_columns]).value_counts()\n",
    "\n",
    "# Print results\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(\"Repeats per group:\")\n",
    "print(group_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fcb51-5eb0-4397-bf88-0b8421c9c2b3",
   "metadata": {},
   "source": [
    "#Check FDR distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3257ad-b107-4739-a433-1376ea45e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of proteins with High, Medium, and Low FDR confidence levels.\n",
    "\n",
    "# Count the number of occurrences for each FDR confidence level\n",
    "fdr_counts = data['Protein.FDR.Confidence..Combined'].value_counts()\n",
    "\n",
    "# Calculate the total number of entries\n",
    "total_proteins = fdr_counts.sum()\n",
    "\n",
    "# Calculate the percentage for each FDR confidence level\n",
    "fdr_percentage = (fdr_counts / total_proteins) * 100\n",
    "\n",
    "# Display the results\n",
    "fdr_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bb109-2c09-4468-ad7a-d34973537517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path to the preprocess folder\n",
    "folder_path = \"./\"\n",
    "file_name = \"FDR_Distribution_1.pdf\"\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Count proteins by FDR confidence levels\n",
    "fdr_counts = data['Protein.FDR.Confidence..Combined'].value_counts()\n",
    "\n",
    "# Labels and sizes for the pie chart\n",
    "labels = fdr_counts.index\n",
    "sizes = fdr_counts.values\n",
    "colors = [\"#6667ff\", \"#66b3ff\", \"#66ffff\"]\n",
    "\n",
    "# Create a pie chart with a center circle (donut chart)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors,\n",
    "        wedgeprops=dict(width=0.6), pctdistance=0.89, \n",
    "        textprops={'fontsize': 12, 'fontweight': 'bold'})  # Adjusted font size for labels\n",
    "\n",
    "# Draw a smaller circle at the center to make it look like a donut\n",
    "centre_circle = plt.Circle((0, 0), 0.30, fc='white')  # Adjusted size of the center circle\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Set the title with the specified font size and bold styling\n",
    "plt.title('', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Display the plot\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie is drawn as a circle\n",
    "\n",
    "# Save the plot as a PDF\n",
    "plt.savefig(file_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289fe17-b46d-4d4f-820b-74349e96db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in the entire dataset\n",
    "missing_values_percent = data.isnull().mean().mean() * 100\n",
    "\n",
    "# Print the percentage of missing values\n",
    "print(f\"Percentage of missing values in the dataset: {missing_values_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9ece9-2f77-42c8-9d84-c8baeff5b5d5",
   "metadata": {},
   "source": [
    "## Filter Data based on -> ([FDR] == \"High\") & [PSMs] >= 2) & [Coverage] >= 20) criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6403695-43fe-4620-aabd-3a76dec8b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data for FDR == \"high\", PSMs >= 2, and coverage >= 20\n",
    "folder_path = \"./\"\n",
    "\n",
    "FDR=\"Protein.FDR.Confidence..Combined\"\n",
    "\n",
    "filtered_df = data[(data[FDR] == \"High\") & (data['PSMs'] >= 2) & (data['Coverage'] >= 20)]\n",
    "\n",
    "# Specify the full file path for the output CSV file\n",
    "output_path = \"./filtered_FDR.csv\"\n",
    "\n",
    "# Save the filtered DataFrame to the specified path\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8249ed-b04d-4f2b-bea2-1f2ef7157bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in the entire dataset\n",
    "missing_values_percent = filtered_df.isnull().mean().mean() * 100\n",
    "\n",
    "# Print the percentage of missing values\n",
    "print(f\"Percentage of missing values in the dataset: {missing_values_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f473d6f8-fe05-4caf-a491-8a9ca6bb5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### missingno and check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf5f95-e330-4f46-853d-16b11677e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./\"\n",
    "matrix_file = \"missing_matrix_1.pdf\"\n",
    "bar_file = \"missing_bar_1.pdf\"\n",
    "\n",
    "# Select columns 7 to 33 (Python indexing is 0-based, and iloc excludes the last index)\n",
    "selected_columns = filtered_df.iloc[:, 6:33]\n",
    "\n",
    "# Plot 1: Missing values matrix\n",
    "msno.matrix(selected_columns)\n",
    "matrix_output_path = folder_path + matrix_file\n",
    "plt.savefig(matrix_output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Missing value matrix saved as: {matrix_output_path}\")\n",
    "\n",
    "# Plot 2: Missing values bar chart\n",
    "msno.bar(selected_columns)\n",
    "bar_output_path = folder_path + bar_file\n",
    "plt.savefig(bar_output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Missing value bar chart saved as: {bar_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bcdf6af-147b-4427-a8b2-bebc0b7f1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## speprate Check for Data Availability : at least 2/3 of replicates per groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbf025-d5b3-445c-b231-722a6a094518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Re-define the groups and replicates\n",
    "groups = {\n",
    "    'WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'BtgLPS': ['BtgLPS5_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Step 3: Create a function to check if more than 66% of data is missing in each group\n",
    "def calculate_missing_percentage(row, groups):\n",
    "    for group, columns in groups.items():\n",
    "        valid_columns = [col for col in columns if col in row.index]\n",
    "        if valid_columns:\n",
    "            missing_percentage = row[valid_columns].isnull().mean() * 100\n",
    "            if missing_percentage > 66:\n",
    "                return True  # This row has more than 66% missing values in at least one group\n",
    "    return False\n",
    "\n",
    "# Step 4: Apply the function to classify each row\n",
    "filtered_df['more_than_66_missing'] = filtered_df.apply(calculate_missing_percentage, axis=1, groups=groups)\n",
    "\n",
    "# Step 5: Split the data into two groups\n",
    "more_than_66_df = filtered_df[filtered_df['more_than_66_missing']]\n",
    "less_than_or_equal_66_df = filtered_df[~filtered_df['more_than_66_missing']]\n",
    "\n",
    "# Step 6: Save both datasets as separate CSV files\n",
    "more_than_66_file = \"./more_than_66.csv\"\n",
    "less_than_or_equal_66_file = \"./less_than_or_equal_66.csv\"\n",
    "\n",
    "more_than_66_df.to_csv(more_than_66_file, index=False)\n",
    "less_than_or_equal_66_df.to_csv(less_than_or_equal_66_file, index=False)\n",
    "\n",
    "(more_than_66_file, less_than_or_equal_66_file, len(more_than_66_df), len(less_than_or_equal_66_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d2d82-d1d8-411c-a4fd-cdec84280a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58f9212-eded-450c-8503-0354b5cd54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imputing missing data in data_less_66 by KNN techniques and check After imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60031a39-b9c4-416d-9cca-8c3b4a4f0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the `less_than_or_equal_66.csv` file\n",
    "file_path_2 = \"./less_than_or_equal_66.csv\"\n",
    "data_less_66 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Define the 8 groups of columns\n",
    "group_definitions = {\n",
    "    'Group_1_WtSham': [ 'WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Function to apply KNN Imputation group-wise\n",
    "def knn_impute_groupwise(data_less_66, group_definitions, n_neighbors=3):\n",
    "    # Create a copy of the original data\n",
    "    imputed_data = data_less_66.copy()\n",
    "    \n",
    "    # Loop over each group\n",
    "    for group_name, group_columns in group_definitions.items():\n",
    "        print(f\"Imputing group: {group_name}\")\n",
    "        \n",
    "        # Select the data corresponding to the current group\n",
    "        group_data = data[group_columns]\n",
    "        \n",
    "        # Apply KNN Imputation to this group\n",
    "        knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_group_data = pd.DataFrame(knn_imputer.fit_transform(group_data), columns=group_columns)\n",
    "        \n",
    "        # Replace the imputed data back into the original dataframe\n",
    "        imputed_data[group_columns] = imputed_group_data\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "# Apply KNN imputation group-wise\n",
    "imputed_data = knn_impute_groupwise(data_less_66, group_definitions)\n",
    "\n",
    "# Save the imputed data to a CSV file (optional)\n",
    "output_file_path = \"./imputed_data.csv\"\n",
    "imputed_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Imputation completed and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c54ce8-f68e-4f4d-b40e-112524282794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset \n",
    "file_path = \"./less_than_or_equal_66.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the 8 groups of columns\n",
    "group_definitions = {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Function to apply KNN Imputation group-wise\n",
    "def knn_impute_groupwise(data, group_definitions, n_neighbors=3):\n",
    "    # Create a copy of the original data\n",
    "    imputed_data = data.copy()\n",
    "    \n",
    "    # Loop over each group\n",
    "    for group_name, group_columns in group_definitions.items():\n",
    "        print(f\"Imputing group: {group_name}\")\n",
    "        \n",
    "        # Check if all columns in the group exist in the data\n",
    "        missing_columns = [col for col in group_columns if col not in data.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {group_name} due to missing columns: {missing_columns}\")\n",
    "            continue\n",
    "        \n",
    "        # Select the data corresponding to the current group\n",
    "        group_data = data[group_columns]\n",
    "        \n",
    "        # Apply KNN Imputation to this group\n",
    "        knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_group_data = pd.DataFrame(knn_imputer.fit_transform(group_data), columns=group_columns)\n",
    "        \n",
    "        # Replace the imputed data back into the original dataframe\n",
    "        imputed_data[group_columns] = imputed_group_data\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "# Apply KNN imputation group-wise\n",
    "imputed_data = knn_impute_groupwise(data, group_definitions)\n",
    "\n",
    "# Save the imputed data to a CSV file\n",
    "output_file_path = \"./imputed_data.csv\"\n",
    "imputed_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Imputation completed and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6f6fa97-fe67-4d3b-bf16-e73721d6adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcd840-5749-42a1-89eb-f7f1d6f83d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "data_before = pd.read_csv(\"./less_than_or_equal_66.csv\")\n",
    "data_after = pd.read_csv(\"./imputed_data.csv\")\n",
    "\n",
    "# Select only columns 8 to 34 (index 6 to 33)\n",
    "data_before_selected = data_before.iloc[:, 6:33]\n",
    "data_after_selected = data_after.iloc[:, 6:33]\n",
    "\n",
    "# Count the missing values before and after imputation\n",
    "missing_values_before = data_before_selected.isnull().sum().sum()\n",
    "missing_values_after = data_after_selected.isnull().sum().sum()\n",
    "\n",
    "# Create a bar plot to compare missing values before and after imputation\n",
    "labels = ['Before Imputation', 'After Imputation']\n",
    "values = [missing_values_before, missing_values_after]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, values, color=['#ff6666', '#66b3ff'])\n",
    "plt.title('Missing Values Before and After Imputation', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Number of Missing Values', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./missing_values_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Create heatmaps to visualize missing values\n",
    "# Heatmap BEFORE imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(data_before_selected.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
    "plt.title('Missing Values Before Imputation')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./before_impute_heatmap.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmap AFTER imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(data_after_selected.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
    "plt.title('Missing Values After Imputation')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./after_impute_heatmap.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total missing values before imputation: {missing_values_before}\")\n",
    "print(f\"Total missing values after imputation: {missing_values_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96477e9b-a84e-4f21-8337-7b4e4a1f609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset before and after imputation using relative paths\n",
    "file_path_before = \"./less_than_or_equal_66.csv\"\n",
    "file_path_after = \"./imputed_data.csv\"\n",
    "before_imputation_data = pd.read_csv(file_path_before)\n",
    "after_imputation_data = pd.read_csv(file_path_after)\n",
    "\n",
    "# Step 1: Select only columns 7 to 34 \n",
    "numeric_before_imputation = before_imputation_data.iloc[:, 6:33]\n",
    "numeric_after_imputation = after_imputation_data.iloc[:, 6:33]\n",
    "\n",
    "# Step 2: Select only non-missing values from the 'before' dataset\n",
    "non_missing_before_imputation = numeric_before_imputation.dropna()\n",
    "\n",
    "# Step 3: Apply log2 transformation (with a small constant to avoid log(0))\n",
    "log2_non_missing_before = np.log2(non_missing_before_imputation + 1e-9)\n",
    "log2_after_imputation = np.log2(numeric_after_imputation + 1e-9)\n",
    "\n",
    "# Step 4: Create visualizations to compare the distributions\n",
    "\n",
    "# a) Histogram + KDE Comparison\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot histogram of non-missing values before imputation\n",
    "sns.histplot(log2_non_missing_before.stack(), bins=50, kde=True, color='blue', alpha=0.5, label='Before Imputation (Non-missing)')\n",
    "\n",
    "# Plot histogram of all values after imputation\n",
    "sns.histplot(log2_after_imputation.stack(), bins=50, kde=True, color='green', alpha=0.5, label='After Imputation (All Data)')\n",
    "\n",
    "# Enhancements for font size and weight\n",
    "plt.title('Histogram and KDE Comparison of Data Before and After Imputation (Log2 Transformed)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Log2 Values', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=12, frameon=False)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Set tick labels to be bold and font size 12\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "# Save the plot as a PDF in the current directory\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./before_and_after_impute_dis.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835179a-c363-4c23-bd25-7043b0df3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"./imputed_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Select only columns 7 to 34\n",
    "data_selected = data.iloc[:, 6:33]\n",
    "\n",
    "# Step 2: Apply log2 transformation (with a small constant to avoid log(0))\n",
    "log2_transformed_data = np.log2(data_selected + 1e-9)\n",
    "\n",
    "# Step 3a: Box plot for log2 transformed data (before normalization)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=log2_transformed_data)\n",
    "plt.title('Box Plot of Log2 Transformed Data (Before Normalization)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./before_normalization.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Simulate median normalization (replace with your actual normalization method)\n",
    "median_normalized_data = log2_transformed_data.subtract(log2_transformed_data.median(axis=0), axis=1)\n",
    "\n",
    "# Step 3b: Box plot for median-normalized data (after normalization)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=median_normalized_data)\n",
    "plt.title('Box Plot of Median Normalized Data (After Normalization)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./after_normalization.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782dfa7-8a33-447d-aa2f-89d11b5966c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"./imputed_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Select only columns 8 to 27 for log2 transformation and normalization\n",
    "data_selected = data.iloc[:, 6:33]\n",
    "\n",
    "# Step 2: Apply log2 transformation (with a small constant to avoid log(0))\n",
    "log2_transformed_data = np.log2(data_selected + 1e-9)\n",
    "\n",
    "# Step 3: Perform median normalization\n",
    "median_normalized_data = log2_transformed_data.subtract(log2_transformed_data.median(axis=0), axis=1)\n",
    "\n",
    "# Step 4: Replace the original columns (8 to 27) with the normalized data\n",
    "data.iloc[:, 6:33] = median_normalized_data\n",
    "\n",
    "# Step 5: Remove the last column from the dataset\n",
    "data = data.iloc[:, :-1]\n",
    "\n",
    "# Step 6: Save the full dataset (with normalized columns and without the last column) as Final_Normalized.csv\n",
    "data.to_csv(\"./Final_Normalized.csv\", index=False)\n",
    "print(\"Final normalized data has been saved as 'Final_Normalized.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e01ba-834e-400e-a7c1-e30b89077d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract columns 7 to 34 (Python index 6 to 33)\n",
    "data_columns_7_34 = data.iloc[:, 6:33]\n",
    "\n",
    "# 1. Check column medians\n",
    "medians = data_columns_7_34.median()\n",
    "print(\"Column Medians:\")\n",
    "print(medians)\n",
    "\n",
    "# 2. Generate histograms\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(data_columns_7_34.columns):\n",
    "    plt.subplot(5, 6, i + 1)\n",
    "    plt.hist(data_columns_7_34[column], bins=30, alpha=0.7, edgecolor='k')\n",
    "    plt.title(column, fontsize=8)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "histogram_pdf_filename = \"histograms_output.pdf\"\n",
    "plt.savefig(histogram_pdf_filename, format=\"pdf\")\n",
    "plt.close()\n",
    "print(f\"Histograms saved as {histogram_pdf_filename}\")\n",
    "\n",
    "# 3. Generate Q-Q plots\n",
    "num_columns = len(data_columns_7_34.columns)\n",
    "num_rows = (num_columns // 5) + 1\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(data_columns_7_34.columns):\n",
    "    plt.subplot(num_rows, 5, i + 1)\n",
    "    stats.probplot(data_columns_7_34[column].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title(column, fontsize=8)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "qq_plot_pdf_filename = \"qq_plots_output.pdf\"\n",
    "plt.savefig(qq_plot_pdf_filename, format=\"pdf\")\n",
    "plt.close()\n",
    "print(f\"Q-Q Plots saved as {qq_plot_pdf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfdf7916-a6be-4944-86d6-7e14a058b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check normality of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9c7e0-581f-4fd1-b6bf-bd2497988c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the uploaded file for accuracy\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract columns 7 to 34 (Python index 6 to 33) for the check\n",
    "data_columns_7_34_corrected = data.iloc[:, 6:33]\n",
    "\n",
    "# 1. Check column medians to ensure they are close to zero\n",
    "medians = data_columns_7_34_corrected.median()\n",
    "\n",
    "# 2. Visualize histograms for all columns to check distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(data_columns_7_34_corrected.columns):\n",
    "    plt.subplot(5, 6, i + 1)\n",
    "    plt.hist(data_columns_7_34_corrected[column], bins=30, alpha=0.7, edgecolor='k')\n",
    "    plt.title(column)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure as a PDF\n",
    "pdf_filename = \"histograms_output.pdf\"\n",
    "plt.savefig(pdf_filename, format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Histograms saved as {pdf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a42d0-0d4b-4c60-9bc6-aa04feb50abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449ad24-765e-4f4a-a9a9-c3093d862923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract columns 7 to 34 (Python index 6 to 33) for the check\n",
    "data_columns_final_normalized = data.iloc[:, 6:33]\n",
    "\n",
    "# Generate Q-Q plots for all columns in data_columns_final_normalized\n",
    "num_columns = len(data_columns_final_normalized.columns)\n",
    "num_rows = (num_columns // 5) + 1\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(data_columns_final_normalized.columns):\n",
    "    plt.subplot(num_rows, 5, i + 1)\n",
    "    stats.probplot(data_columns_final_normalized[column].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title(column, fontsize=8)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "182a8177-9867-497e-9e58-2339e343c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "### shapiro _wilk normality test for each sample normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132391e0-5a6a-43af-82c6-84e31018635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the dataset since the environment reset\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Re-define groups\n",
    "groups = {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Function to check normality for each sample in each group using Shapiro-Wilk test\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "normality_results = {}\n",
    "\n",
    "for group_name, samples in groups.items():\n",
    "    normality_results[group_name] = {}\n",
    "    for sample in samples:\n",
    "        stat, p_value = shapiro(data[sample].dropna())\n",
    "        normality_results[group_name][sample] = {'W-statistic': stat, 'p-value': p_value}\n",
    "\n",
    "# Convert the results to a DataFrame for easier interpretation\n",
    "normality_df = pd.DataFrame.from_dict({(i, j): normality_results[i][j]\n",
    "                                       for i in normality_results.keys()\n",
    "                                       for j in normality_results[i].keys()},\n",
    "                                      orient='index')\n",
    "\n",
    "\n",
    "# Display the first few rows of the results\n",
    "print(normality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a715df-2f40-4d05-8cbe-7cb02f81961f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your data (assuming it's already loaded as `data`)\n",
    "\n",
    "# Step 1: Extract and scale the data (already log2-transformed and median-subtracted)\n",
    "data_columns_final_normalized = data.iloc[:, 6:33]  # Select appropriate columns for analysis\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_columns_final_normalized.T)\n",
    "\n",
    "# Step 2: Apply UMAP\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric='euclidean', random_state=42)\n",
    "umap_result = reducer.fit_transform(scaled_data)\n",
    "\n",
    "# Step 3: Prepare group labels (ensure it matches the number of samples)\n",
    "group_labels = [\n",
    "    'Group_1', 'Group_1', 'Group_1',   # Group 1 - WtSham\n",
    "    'Group_2', 'Group_2', 'Group_2', 'Group_2',  # Group 2 - WtLPS \n",
    "    'Group_3', 'Group_3', 'Group_3', 'Group_3',  # Group 3 - WtLPSTB\n",
    "    'Group_4', 'Group_4', 'Group_4',   # Group 4 - WtShamTB\n",
    "    'Group_5', 'Group_5', 'Group_5',   # Group 5 - BtgSham\n",
    "    'Group_6', 'Group_6', 'Group_6', 'Group_6',  # Group 6 - BtgLPS\n",
    "    'Group_7', 'Group_7', 'Group_7',   # Group 7 - BKSham\n",
    "    'Group_8', 'Group_8', 'Group_8'    # Group 8 - BKLPS\n",
    "]\n",
    "\n",
    "# Ensure group_labels length matches the number of rows in umap_result\n",
    "if len(group_labels) != umap_result.shape[0]:\n",
    "    raise ValueError(f\"Length of group_labels ({len(group_labels)}) does not match the number of samples ({umap_result.shape[0]}).\")\n",
    "\n",
    "# Step 4: Convert UMAP results into a DataFrame\n",
    "umap_df = pd.DataFrame(umap_result, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['Group'] = group_labels\n",
    "\n",
    "# Step 5: Plot UMAP results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='UMAP1', y='UMAP2', hue='Group', data=umap_df, \n",
    "    palette='Set2', s=100, alpha=0.8, edgecolor='k'\n",
    ")\n",
    "plt.title('UMAP for Batch Effect Detection')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Group\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Save the UMAP plot as a PDF\n",
    "umap_pdf_filename = \"umap_batch_effect.pdf\"\n",
    "plt.savefig(umap_pdf_filename, format=\"pdf\")\n",
    "plt.show()\n",
    "print(f\"UMAP plot saved as {umap_pdf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f674acd-9b43-455c-9787-df0f942c6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Re-create scaled data as it might have been cleared from memory\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_columns_final_normalized.T)\n",
    "\n",
    "# Step 1: Apply PCA to the scaled data\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Step 2: Convert PCA results into a DataFrame\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['Group'] = group_labels\n",
    "\n",
    "# Step 3: Plot the PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2', hue='Group', data=pca_df, \n",
    "    palette='Set2', s=100, alpha=0.8, edgecolor='k'\n",
    ")\n",
    "plt.title('PCA for Batch Effect Detection')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% variance)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Group\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the UMAP plot as a PDF\n",
    "umap_pdf_filename = \"umap_batch_effect_2.pdf\"\n",
    "plt.savefig(umap_pdf_filename, format=\"pdf\")\n",
    "plt.show()\n",
    "print(f\"UMAP plot saved as {umap_pdf_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855354d-a081-4aff-85bb-5a7adb80d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your data (assuming it's already loaded as `data`)\n",
    "data_columns_final_normalized = data.iloc[:, 6:33]  # Adjust the column range if needed\n",
    "\n",
    "# Step 1: Scale the data (already log2-transformed and median-subtracted)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_columns_final_normalized.T)\n",
    "\n",
    "# Step 2: Apply UMAP\n",
    "reducer = umap.UMAP(n_neighbors=6, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "umap_result = reducer.fit_transform(scaled_data)\n",
    "\n",
    "# Step 3: Prepare group labels with descriptive names\n",
    "group_labels = [\n",
    "    'WtSham', 'WtSham', 'WtSham',\n",
    "    'WtLPS', 'WtLPS', 'WtLPS', 'WtLPS',\n",
    "    'WtLPSTB', 'WtLPSTB', 'WtLPSTB', 'WtLPSTB',\n",
    "    'WtShamTB', 'WtShamTB', 'WtShamTB',\n",
    "    'BtgSham', 'BtgSham', 'BtgSham',\n",
    "    'BtgLPS', 'BtgLPS', 'BtgLPS', 'BtgLPS',\n",
    "    'BKSham', 'BKSham', 'BKSham',\n",
    "    'BKLPS', 'BKLPS', 'BKLPS'\n",
    "]\n",
    "\n",
    "# Ensure group_labels length matches the number of rows in umap_result\n",
    "if len(group_labels) != umap_result.shape[0]:\n",
    "    raise ValueError(f\"Length of group_labels ({len(group_labels)}) does not match the number of samples ({umap_result.shape[0]}).\")\n",
    "\n",
    "# Step 4: Convert UMAP results into a DataFrame\n",
    "umap_df = pd.DataFrame(umap_result, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['Group'] = group_labels\n",
    "\n",
    "# Save the UMAP plot as a PDF\n",
    "\n",
    "\n",
    "# Step 5: Plot UMAP with sample group names as labels\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(\n",
    "    x='UMAP1', y='UMAP2', hue='Group', data=umap_df, \n",
    "    palette='Set2', s=150, alpha=0.8, edgecolor='k')\n",
    "\n",
    "plt.title('UMAP for Batch Effect Detection (with Group Names)')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Group\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "# Save the figure with a larger page size and tight bounding box\n",
    "umap_pdf_filename = \"umap_batch_effect_large.pdf\"\n",
    "plt.savefig(umap_pdf_filename, format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"UMAP plot saved as {umap_pdf_filename}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"UMAP plot saved as {umap_pdf_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b591cf4-46b8-48da-bb0b-cd71ce07fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the sample groups\n",
    "groups = {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Melt the data to long format for ANOVA\n",
    "sample_columns = [col for group in groups.values() for col in group]\n",
    "melted_data = data[sample_columns].melt(var_name=\"Sample\", value_name=\"Expression\")\n",
    "\n",
    "# Create a batch variable based on the group each sample belongs to\n",
    "batch_mapping = {sample: group for group, samples in groups.items() for sample in samples}\n",
    "melted_data[\"Batch\"] = melted_data[\"Sample\"].map(batch_mapping)\n",
    "\n",
    "# Step 2: Perform ANOVA to test for batch effects\n",
    "model = ols(\"Expression ~ C(Batch)\", data=melted_data).fit()\n",
    "anova_results = anova_lm(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display ANOVA results\n",
    "anova_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b154b4cf-4368-4340-919a-bcb77b3d727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the P value > 0.05 and batch effect is not statisticaly significant. NO batch effect in data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480750ea-f372-4b46-81f2-541ea57e37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Extract only the sample columns for PCA analysis\n",
    "sample_columns = [\n",
    "    'WtSham_2', 'WtSham_3', 'WtSham_4', 'WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4', \n",
    "    'WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4', 'WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3', \n",
    "    'BtgSham_1', 'BtgSham_2', 'BtgSham_3', 'BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4', \n",
    "    'BKSham_1', 'BKSham_2', 'BKSham_3', 'BKLPS_1', 'BKLPS_2', 'BKLPS_3'\n",
    "]\n",
    "\n",
    "# Prepare the data for PCA by dropping rows with any missing values in these columns\n",
    "data_pca = data[sample_columns].dropna()\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_pca.T)\n",
    "\n",
    "# Create a DataFrame for the PCA result\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['Sample'] = sample_columns\n",
    "\n",
    "# Generate a heatmap of the sample correlations to check for batch effects\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(data_pca.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Heatmap of Sample Correlation')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9ebca-1fa6-4b48-9053-39d046459d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd3427-106b-48d8-ae7f-88d834b2ac2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant columns (all columns representing samples)\n",
    "sample_columns = data.columns[6:33]  # Adjust based on the actual columns containing sample data\n",
    "\n",
    "# Transpose for distance calculations\n",
    "data_matrix = data[sample_columns].T.to_numpy()\n",
    "\n",
    "# Step 1: Calculate Euclidean distances for all columns\n",
    "euclidean_distances = pdist(data_matrix, metric='euclidean')\n",
    "euclidean_distances_matrix = squareform(euclidean_distances)\n",
    "\n",
    "# Step 2: Plot the Euclidean distance heatmap for all columns\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(euclidean_distances_matrix, annot=True, xticklabels=sample_columns, yticklabels=sample_columns, cmap=\"YlGnBu\")\n",
    "plt.title(\"Euclidean Distances Between All Samples\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Save the plot as a PDF\n",
    "output_path = \"./Euclidean_Distances_All_Samples.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Provide the location of the saved PDF\n",
    "print(f\"The plot has been saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98aea04-dc2c-4396-acb9-949329ba57f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3027ac0-8c69-407e-aa87-14ea830de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "file_path = \"./Final_Normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#Define groups based on available sample columns\n",
    "# Define the 8 groups of columns\n",
    "groups= {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Compute group means\n",
    "group_means = {}\n",
    "for group, samples in groups.items():\n",
    "    group_means[group] = data[samples].mean(axis=1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "group_means_df = pd.DataFrame(group_means)\n",
    "\n",
    "# Transpose for distance calculations\n",
    "group_means_matrix = group_means_df.T.to_numpy()\n",
    "\n",
    "# Calculate Euclidean distances\n",
    "euclidean_distances = pdist(group_means_matrix, metric='euclidean')\n",
    "euclidean_distances_matrix = squareform(euclidean_distances)\n",
    "\n",
    "# Plot Euclidean distances heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(euclidean_distances_matrix, annot=True, xticklabels=group_means_df.columns, yticklabels=group_means_df.columns, cmap=\"YlGnBu\")\n",
    "plt.title(\"Euclidean Distances Between Groups\")\n",
    "\n",
    "\n",
    "# Save the plot as a PDF\n",
    "output_path = \"./Eucleadian distance.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Provide the location of the saved PDF\n",
    "output_path\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1533e-1900-4942-97b6-746b9126ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the groups based on available sample columns\n",
    "# Define the 8 groups of columns\n",
    "groups = {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# Check that the groups match the column names in the DataFrame\n",
    "for group, samples in groups.items():\n",
    "    missing_cols = [col for col in samples if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns are missing for group {group}: {missing_cols}\")\n",
    "\n",
    "# Compute group means (average intensity across samples for each group)\n",
    "group_means = {}\n",
    "for group, samples in groups.items():\n",
    "    group_means[group] = data[samples].mean(axis=1)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "group_means_df = pd.DataFrame(group_means)\n",
    "\n",
    "# Transpose the group means matrix for distance calculations\n",
    "group_means_matrix = group_means_df.T.to_numpy()\n",
    "\n",
    "# Calculate Euclidean distances between group means\n",
    "euclidean_distances = pdist(group_means_matrix, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical clustering using linkage\n",
    "linkage_matrix = linkage(euclidean_distances, method='average')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, labels=group_means_df.columns, orientation='top', leaf_rotation=90)\n",
    "plt.title('Dendrogram of Group Similarities (Euclidean Distance)')\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF (in the current working directory for now)\n",
    "output_path_dendrogram = \"Dendrogram_Euclidean_distance.pdf\"\n",
    "plt.savefig(output_path_dendrogram, format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the location of the saved PDF\n",
    "print(f\"Dendrogram plot saved at: {output_path_dendrogram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed2e13-4aee-4b61-9562-00b4df4e0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the 8 groups of columns\n",
    "groups = {\n",
    "    'Group_1_WtSham': ['WtSham_2', 'WtSham_3', 'WtSham_4'],\n",
    "    'Group_2_WtLPS': ['WtLPS_1', 'WtLPS_2', 'WtLPS_3', 'WtLPS_4'],\n",
    "    'Group_3_WtLPSTB': ['WtLPSTB_1', 'WtLPSTB_2', 'WtLPSTB_3', 'WtLPSTB_4'],\n",
    "    'Group_4_WtShamTB': ['WtShamTB_1', 'WtShamTB_2', 'WtShamTB_3'],\n",
    "    'Group_5_BtgSham': ['BtgSham_1', 'BtgSham_2', 'BtgSham_3'],\n",
    "    'Group_6_BtgLPS': ['BtgLPS_1', 'BtgLPS_2', 'BtgLPS_3', 'BtgLPS_4'],\n",
    "    'Group_7_BKSham': ['BKSham_1', 'BKSham_2', 'BKSham_3'],\n",
    "    'Group_8_BKLPS': ['BKLPS_1', 'BKLPS_2', 'BKLPS_3']\n",
    "}\n",
    "\n",
    "# List to store mean values for each group\n",
    "group_means = {}\n",
    "\n",
    "# Loop through each group and calculate the mean for each protein\n",
    "for group_name, group_columns in group_definitions.items():\n",
    "    group_means[group_name] = data[group_columns].mean(axis=1)\n",
    "\n",
    "# Convert the group means dictionary into a DataFrame for easier handling\n",
    "group_means_df = pd.DataFrame(group_means)\n",
    "\n",
    "# Create a clustermap with hierarchical clustering\n",
    "plt.figure(figsize=(14, 12))  # Increase the figure size for better spacing\n",
    "clustermap=sns.clustermap(group_means_df, annot=False, cmap='coolwarm', cbar_kws={'label': 'Log2 Normalized Protein Abundance'},\n",
    "               xticklabels=group_means_df.columns, yticklabels=False, method='complete', metric='euclidean',\n",
    "               dendrogram_ratio=(0.1, 0.34), figsize=(14, 12), tree_kws={'linewidths': 2})  # Increased linewidth for tree\n",
    "\n",
    "# Set title and labels for clarity\n",
    "plt.title(\"\", fontsize=15)\n",
    "plt.xlabel(\" \", fontsize=12)\n",
    "plt.ylabel(\"Mean log2-normalized Abundance\", fontsize=12,rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the clustermap as a high-resolution PDF\n",
    "clustermap_pdf_filename = \"protein_clustermap.pdf\"\n",
    "clustermap.fig.savefig(clustermap_pdf_filename, format=\"pdf\", bbox_inches=\"tight\", dpi=300)  # Use `.fig` to access figure\n",
    "\n",
    "print(f\"Clustermap saved as {clustermap_pdf_filename}\")\n",
    "-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ddddee-4112-4783-ad9d-f108e12ea7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
